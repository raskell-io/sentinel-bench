#!/usr/bin/env bash
#MISE description="Run benchmark against a proxy"
#MISE alias="b"
#MISE usage="bench <proxy> <scenario> [options]"

set -euo pipefail

# =============================================================================
# Sentinel Benchmarking Framework - Main Runner
# Usage: mise run bench <proxy> <scenario> [options]
#
# Results are stored in: results/<timestamp>_<proxy>_<scenario>/
# Each run includes comprehensive metadata for analytics and GitHub Pages.
# =============================================================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ROOT_DIR="$(cd "$SCRIPT_DIR/../.." && pwd)"
RESULTS_DIR="$ROOT_DIR/results"

# Use environment variables with defaults
DURATION="${BENCH_DURATION:-60}"
CONNECTIONS="${BENCH_CONNECTIONS:-100}"
THREADS="${BENCH_THREADS:-4}"
RATE="${BENCH_RATE:-10000}"
WARMUP="${BENCH_WARMUP:-10}"
RUNS="${BENCH_RUNS:-3}"
TARGET="${BENCH_TARGET:-http://localhost:8080}"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
BOLD='\033[1m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1" >&2; }
log_success() { echo -e "${GREEN}[OK]${NC} $1" >&2; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1" >&2; }
log_error() { echo -e "${RED}[ERROR]${NC} $1" >&2; }

usage() {
    cat <<EOF
Sentinel Benchmarking Framework

Usage: mise run bench <proxy> <scenario> [options]

Proxies:
  sentinel    Sentinel reverse proxy
  envoy       Envoy proxy
  haproxy     HAProxy
  nginx       Nginx
  all         Run all proxies (comparison mode)

Scenarios:
  passthrough Basic L7 passthrough (no processing)
  routing     Multiple routes, path matching
  tls         HTTPS termination
  loadbalance Multiple backends

Options:
  --duration <secs>    Test duration (default: $DURATION)
  --connections <n>    Concurrent connections (default: $CONNECTIONS)
  --threads <n>        Worker threads (default: $THREADS)
  --rate <rps>         Target requests/second (default: $RATE)
  --warmup <secs>      Warmup duration (default: $WARMUP)
  --runs <n>           Number of runs (default: $RUNS)
  --target <url>       Target URL (default: $TARGET)
  --tool <oha|wrk2|wrk|hey|k6>  Load generator (default: auto-detect)
  --name <string>      Optional name/tag for this benchmark run
  --help               Show this help

Results are stored in: results/<timestamp>_<proxy>_<scenario>/

EOF
    exit 0
}

# =============================================================================
# Environment Detection
# =============================================================================

get_cpu_info() {
    if [[ "$OSTYPE" == "darwin"* ]]; then
        sysctl -n machdep.cpu.brand_string 2>/dev/null || echo "Unknown"
    else
        grep "model name" /proc/cpuinfo 2>/dev/null | head -1 | cut -d: -f2 | xargs || echo "Unknown"
    fi
}

get_cpu_cores() {
    if [[ "$OSTYPE" == "darwin"* ]]; then
        sysctl -n hw.ncpu 2>/dev/null || echo "0"
    else
        nproc 2>/dev/null || echo "0"
    fi
}

get_memory_gb() {
    if [[ "$OSTYPE" == "darwin"* ]]; then
        local bytes
        bytes=$(sysctl -n hw.memsize 2>/dev/null || echo "0")
        echo $((bytes / 1024 / 1024 / 1024))
    else
        free -g 2>/dev/null | awk '/^Mem:/{print $2}' || echo "0"
    fi
}

get_machine_model() {
    if [[ "$OSTYPE" == "darwin"* ]]; then
        sysctl -n hw.model 2>/dev/null || echo "Unknown"
    else
        cat /sys/class/dmi/id/product_name 2>/dev/null || echo "Unknown"
    fi
}

get_os_info() {
    if [[ "$OSTYPE" == "darwin"* ]]; then
        sw_vers -productName 2>/dev/null || echo "macOS"
    else
        cat /etc/os-release 2>/dev/null | grep "^PRETTY_NAME" | cut -d= -f2 | tr -d '"' || echo "Linux"
    fi
}

get_os_version() {
    if [[ "$OSTYPE" == "darwin"* ]]; then
        sw_vers -productVersion 2>/dev/null || echo "Unknown"
    else
        cat /etc/os-release 2>/dev/null | grep "^VERSION_ID" | cut -d= -f2 | tr -d '"' || echo "Unknown"
    fi
}

get_kernel_version() {
    uname -r 2>/dev/null || echo "Unknown"
}

get_architecture() {
    uname -m 2>/dev/null || echo "Unknown"
}

# =============================================================================
# Proxy Version Detection
# =============================================================================

get_proxy_version() {
    local proxy="$1"
    local version="unknown"

    case "$proxy" in
        sentinel)
            # Try to get version from running container or binary
            version=$(docker exec bench-sentinel sentinel --version 2>/dev/null | head -1 || echo "unknown")
            if [[ "$version" == "unknown" ]]; then
                version=$(sentinel --version 2>/dev/null | head -1 || echo "unknown")
            fi
            ;;
        envoy)
            version=$(docker exec bench-envoy envoy --version 2>/dev/null | grep -oE 'version: [0-9.]+' | cut -d' ' -f2 || echo "unknown")
            if [[ "$version" == "unknown" || -z "$version" ]]; then
                version=$(docker inspect bench-envoy 2>/dev/null | grep -oE 'envoyproxy/envoy:v[0-9.]+' | head -1 | cut -d: -f2 || echo "unknown")
            fi
            ;;
        haproxy)
            version=$(docker exec bench-haproxy haproxy -v 2>/dev/null | head -1 | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
            ;;
        nginx)
            version=$(docker exec bench-nginx nginx -v 2>&1 | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
            ;;
    esac

    echo "$version"
}

get_tool_version() {
    local tool="$1"
    local version="unknown"

    case "$tool" in
        oha)
            version=$(oha --version 2>/dev/null | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' | head -1 || echo "unknown")
            ;;
        wrk2)
            version=$(wrk2 --version 2>/dev/null | head -1 | tr -d '\n' || echo "unknown")
            ;;
        wrk)
            # wrk outputs version info on first line like "wrk 4.2.0 [kqueue]"
            version=$(wrk --version 2>/dev/null | head -1 | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' || echo "unknown")
            ;;
        hey)
            version=$(hey --version 2>/dev/null | head -1 | tr -d '\n' || echo "unknown")
            ;;
        k6)
            version=$(k6 version 2>/dev/null | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+' | head -1 || echo "unknown")
            ;;
    esac

    # Ensure single line, no control characters
    echo "$version" | tr -d '\n\r' | head -c 100
}

# =============================================================================
# Tool Detection
# =============================================================================

detect_tool() {
    if command -v oha &>/dev/null; then
        echo "oha"
    elif command -v wrk2 &>/dev/null; then
        echo "wrk2"
    elif command -v wrk &>/dev/null; then
        log_warn "Using wrk instead of wrk2 - latency may have coordinated omission"
        echo "wrk"
    elif command -v hey &>/dev/null; then
        echo "hey"
    elif command -v k6 &>/dev/null; then
        echo "k6"
    else
        log_error "No load generator found."
        log_error "Install one with: mise run install-oha  OR  mise install"
        exit 1
    fi
}

check_proxy_running() {
    local max_attempts=30
    local attempt=0

    log_info "Waiting for proxy at $TARGET..."

    while ! curl -s -o /dev/null -w "%{http_code}" "$TARGET/" 2>/dev/null | grep -qE "200|404|301|302"; do
        attempt=$((attempt + 1))
        if [ $attempt -ge $max_attempts ]; then
            log_error "Proxy not responding after ${max_attempts}s"
            return 1
        fi
        sleep 1
    done

    log_success "Proxy is ready"
    return 0
}

# Display container details for verification
show_container_info() {
    local proxy="$1"
    local container="bench-${proxy}"

    echo ""
    echo -e "${CYAN}╔══════════════════════════════════════════════════════════════╗${NC}"
    echo -e "${CYAN}║${NC}  ${BOLD}Container Details${NC}"
    echo -e "${CYAN}╠══════════════════════════════════════════════════════════════╣${NC}"

    # Get image name from running container
    local image_name
    image_name=$(docker inspect --format='{{.Config.Image}}' "$container" 2>/dev/null || echo "unknown")
    echo -e "${CYAN}║${NC}  Image:        ${GREEN}${image_name}${NC}"

    # Get image ID
    local image_id
    image_id=$(docker inspect --format='{{.Image}}' "$container" 2>/dev/null | cut -c8-19 || echo "unknown")
    echo -e "${CYAN}║${NC}  Image ID:     ${image_id}"

    # Get architecture from image (container .Platform only returns OS)
    local arch
    arch=$(docker image inspect --format='{{.Architecture}}' "$image_name" 2>/dev/null || echo "unknown")
    echo -e "${CYAN}║${NC}  Architecture: ${YELLOW}${arch}${NC}"

    # Get version from running container
    local version
    version=$(get_proxy_version "$proxy")
    echo -e "${CYAN}║${NC}  Version:      ${version}"

    # Get container status
    local status
    status=$(docker inspect --format='{{.State.Status}}' "$container" 2>/dev/null || echo "unknown")
    echo -e "${CYAN}║${NC}  Status:       ${status}"

    echo -e "${CYAN}╚══════════════════════════════════════════════════════════════╝${NC}"
    echo ""
}

# =============================================================================
# Benchmark Execution
# =============================================================================

run_warmup() {
    local tool="$1"
    log_info "Running warmup for ${WARMUP}s..."

    case "$tool" in
        oha)
            oha -z "${WARMUP}s" -c 50 -q 1000 --no-tui "$TARGET/" >/dev/null 2>&1 || true
            ;;
        wrk2)
            wrk2 -t2 -c50 -d"${WARMUP}s" -R1000 "$TARGET/" >/dev/null 2>&1 || true
            ;;
        wrk)
            wrk -t2 -c50 -d"${WARMUP}s" "$TARGET/" >/dev/null 2>&1 || true
            ;;
        hey)
            hey -z "${WARMUP}s" -c 50 -q 20 "$TARGET/" >/dev/null 2>&1 || true
            ;;
        k6)
            sleep "$WARMUP"
            ;;
    esac

    log_success "Warmup complete"
}

run_benchmark_oha() {
    local output_file="$1"
    local json_file="$2"
    log_info "Running oha: ${CONNECTIONS} connections, ${RATE} RPS, ${DURATION}s"

    # Run with JSON output and save
    oha \
        -z "${DURATION}s" \
        -c "$CONNECTIONS" \
        -q "$RATE" \
        --no-tui \
        --latency-correction \
        --output-format json \
        "$TARGET/" > "$json_file" 2>&1

    # Display summary from JSON
    if [[ -f "$json_file" ]] && command -v jq &>/dev/null; then
        local rps p50 p99
        rps=$(jq -r '.summary.requestsPerSec // .summary.requests_per_sec // 0' "$json_file" 2>/dev/null)
        p50=$(jq -r '.latencyPercentiles.p50 // .responseTimePercentiles.p50 // 0' "$json_file" 2>/dev/null)
        p99=$(jq -r '.latencyPercentiles.p99 // .responseTimePercentiles.p99 // 0' "$json_file" 2>/dev/null)
        echo "Results: ${rps} req/s, p50=${p50}ms, p99=${p99}ms"

        # Save text summary
        jq -r '"Summary:\n  Requests/sec: \(.summary.requestsPerSec // .summary.requests_per_sec // 0)\n  Total requests: \(.summary.total // 0)\n  Latency p50: \(.latencyPercentiles.p50 // .responseTimePercentiles.p50 // 0)ms\n  Latency p99: \(.latencyPercentiles.p99 // .responseTimePercentiles.p99 // 0)ms"' "$json_file" > "$output_file" 2>/dev/null || cp "$json_file" "$output_file"
    else
        echo "JSON output saved to $json_file"
        cp "$json_file" "$output_file"
    fi
}

run_benchmark_wrk2() {
    local output_file="$1"
    local json_file="$2"
    log_info "Running wrk2: ${THREADS} threads, ${CONNECTIONS} connections, ${RATE} RPS, ${DURATION}s"

    wrk2 \
        -t"$THREADS" \
        -c"$CONNECTIONS" \
        -d"${DURATION}s" \
        -R"$RATE" \
        --latency \
        "$TARGET/" 2>&1 | tee "$output_file"

    # Parse wrk2 output (similar format to wrk)
    parse_wrk_output "$output_file" "$json_file"
}

run_benchmark_wrk() {
    local output_file="$1"
    local json_file="$2"
    log_info "Running wrk: ${THREADS} threads, ${CONNECTIONS} connections, ${DURATION}s"

    # Debug: check if wrk is available
    local wrk_path
    wrk_path=$(command -v wrk 2>/dev/null || echo "")
    if [[ -z "$wrk_path" ]]; then
        log_error "wrk not found in PATH"
        log_error "PATH: $PATH"
        return 1
    fi
    log_info "Using wrk at: $wrk_path"

    "$wrk_path" \
        -t"$THREADS" \
        -c"$CONNECTIONS" \
        -d"${DURATION}s" \
        --latency \
        "$TARGET/" 2>&1 | tee "$output_file"

    # Parse wrk output to JSON
    parse_wrk_output "$output_file" "$json_file"
}

# Convert latency values to milliseconds (handle us, ms, s suffixes)
_convert_to_ms() {
    local val="$1"
    if [[ -z "$val" ]]; then
        echo "0"
    elif [[ "$val" == *"us" ]]; then
        echo "scale=3; ${val%us} / 1000" | bc 2>/dev/null || echo "0"
    elif [[ "$val" == *"ms" ]]; then
        echo "${val%ms}"
    elif [[ "$val" == *"s" && "$val" != *"ms" ]]; then
        echo "scale=3; ${val%s} * 1000" | bc 2>/dev/null || echo "0"
    else
        echo "$val"
    fi
}

# Convert RPS values (handle k suffix)
_convert_rps() {
    local val="$1"
    if [[ -z "$val" ]]; then
        echo "0"
    elif [[ "$val" == *"k" ]]; then
        echo "scale=2; ${val%k} * 1000" | bc 2>/dev/null || echo "0"
    else
        echo "$val"
    fi
}

# Parse wrk text output to structured JSON
parse_wrk_output() {
    local input_file="$1"
    local output_file="$2"

    local content
    content=$(cat "$input_file")

    # Extract latency stats (Avg, Stdev, Max) - from "Latency" line in Thread Stats
    local lat_avg lat_stdev lat_max
    lat_avg=$(echo "$content" | grep "Latency" | head -1 | awk '{print $2}')
    lat_stdev=$(echo "$content" | grep "Latency" | head -1 | awk '{print $3}')
    lat_max=$(echo "$content" | grep "Latency" | head -1 | awk '{print $4}')

    # Extract RPS stats - from "Req/Sec" line in Thread Stats
    local rps_avg rps_stdev rps_max
    rps_avg=$(echo "$content" | grep "Req/Sec" | head -1 | awk '{print $2}')
    rps_stdev=$(echo "$content" | grep "Req/Sec" | head -1 | awk '{print $3}')
    rps_max=$(echo "$content" | grep "Req/Sec" | head -1 | awk '{print $4}')

    # Extract percentiles from "Latency Distribution" section
    # These lines look like: "     50%    7.46ms"
    local p50 p75 p90 p99
    p50=$(echo "$content" | grep -E "^\s+50%" | awk '{print $2}')
    p75=$(echo "$content" | grep -E "^\s+75%" | awk '{print $2}')
    p90=$(echo "$content" | grep -E "^\s+90%" | awk '{print $2}')
    p99=$(echo "$content" | grep -E "^\s+99%" | awk '{print $2}')

    # Extract summary stats from line like "130197 requests in 10.00s, 1.67GB read"
    # Duration can be "10.00s" or "1.00m" - convert to seconds
    local total_requests duration_actual data_read duration_raw
    total_requests=$(echo "$content" | grep "requests in" | awk '{print $1}')
    duration_raw=$(echo "$content" | grep "requests in" | awk '{print $4}' | tr -d ',')
    # Convert duration to seconds
    if [[ "$duration_raw" == *"m" ]]; then
        # Minutes - convert to seconds
        duration_actual=$(echo "scale=2; ${duration_raw%m} * 60" | bc 2>/dev/null || echo "0")
    elif [[ "$duration_raw" == *"s" ]]; then
        duration_actual="${duration_raw%s}"
    else
        duration_actual="$duration_raw"
    fi
    data_read=$(echo "$content" | grep "requests in" | awk '{print $5}')

    # Extract final RPS and transfer rate
    local requests_per_sec transfer_per_sec
    requests_per_sec=$(echo "$content" | grep "Requests/sec:" | awk '{print $2}')
    transfer_per_sec=$(echo "$content" | grep "Transfer/sec:" | awk '{print $2}')

    # Extract errors if any (line like "Socket errors: connect 0, read 0, write 0, timeout 0")
    local socket_errors read_errors write_errors timeout_errors
    if echo "$content" | grep -q "Socket errors:"; then
        socket_errors=$(echo "$content" | grep "Socket errors:" | sed 's/.*connect \([0-9]*\).*/\1/')
        read_errors=$(echo "$content" | grep "Socket errors:" | sed 's/.*read \([0-9]*\).*/\1/')
        write_errors=$(echo "$content" | grep "Socket errors:" | sed 's/.*write \([0-9]*\).*/\1/')
        timeout_errors=$(echo "$content" | grep "Socket errors:" | sed 's/.*timeout \([0-9]*\).*/\1/')
    else
        socket_errors=0
        read_errors=0
        write_errors=0
        timeout_errors=0
    fi

    # Convert values
    local lat_avg_ms lat_stdev_ms lat_max_ms p50_ms p75_ms p90_ms p99_ms
    lat_avg_ms=$(_convert_to_ms "$lat_avg")
    lat_stdev_ms=$(_convert_to_ms "$lat_stdev")
    lat_max_ms=$(_convert_to_ms "$lat_max")
    p50_ms=$(_convert_to_ms "$p50")
    p75_ms=$(_convert_to_ms "$p75")
    p90_ms=$(_convert_to_ms "$p90")
    p99_ms=$(_convert_to_ms "$p99")

    local rps_avg_converted rps_stdev_converted rps_max_converted
    rps_avg_converted=$(_convert_rps "$rps_avg")
    rps_stdev_converted=$(_convert_rps "$rps_stdev")
    rps_max_converted=$(_convert_rps "$rps_max")

    cat > "$output_file" <<EOF
{
  "tool": "wrk",
  "latency": {
    "avg_ms": ${lat_avg_ms:-0},
    "stdev_ms": ${lat_stdev_ms:-0},
    "max_ms": ${lat_max_ms:-0},
    "percentiles": {
      "p50_ms": ${p50_ms:-0},
      "p75_ms": ${p75_ms:-0},
      "p90_ms": ${p90_ms:-0},
      "p99_ms": ${p99_ms:-0}
    }
  },
  "throughput": {
    "requests_per_sec": ${requests_per_sec:-0},
    "transfer_per_sec": "${transfer_per_sec:-0}",
    "avg_rps_per_thread": ${rps_avg_converted:-0},
    "stdev_rps": ${rps_stdev_converted:-0},
    "max_rps_per_thread": ${rps_max_converted:-0}
  },
  "summary": {
    "total_requests": ${total_requests:-0},
    "duration_seconds": ${duration_actual:-0},
    "data_read": "${data_read:-0}"
  },
  "errors": {
    "connect": ${socket_errors:-0},
    "read": ${read_errors:-0},
    "write": ${write_errors:-0},
    "timeout": ${timeout_errors:-0}
  }
}
EOF
}

run_benchmark_hey() {
    local output_file="$1"
    local json_file="$2"
    log_info "Running hey: ${CONNECTIONS} connections, ${RATE} RPS, ${DURATION}s"

    hey \
        -z "${DURATION}s" \
        -c "$CONNECTIONS" \
        -q "$((RATE / CONNECTIONS))" \
        "$TARGET/" 2>&1 | tee "$output_file"

    echo '{"tool": "hey", "note": "Parse from txt output"}' > "$json_file"
}

run_benchmark_k6() {
    local output_file="$1"
    local json_file="$2"
    log_info "Running k6: ${CONNECTIONS} VUs, ${DURATION}s"

    k6 run --quiet \
        -e TARGET="$TARGET" \
        -e DURATION="${DURATION}s" \
        -e VUS="$CONNECTIONS" \
        --summary-export="$json_file" \
        - <<'SCRIPT' 2>&1 | tee "$output_file"
import http from 'k6/http';
import { check } from 'k6';

export const options = {
    vus: __ENV.VUS ? parseInt(__ENV.VUS) : 100,
    duration: __ENV.DURATION || '60s',
};

export default function () {
    const res = http.get(__ENV.TARGET + '/');
    check(res, { 'status is 200': (r) => r.status === 200 });
}
SCRIPT
}

# =============================================================================
# Results Aggregation
# =============================================================================

aggregate_results() {
    local result_dir="$1"
    local runs_dir="$result_dir/runs"

    # Skip if no JSON files exist
    if ! ls "$runs_dir"/run_*.json &>/dev/null; then
        log_warn "No run data files found to aggregate"
        return
    fi

    # Use jq if available for proper JSON parsing
    if ! command -v jq &>/dev/null; then
        log_warn "jq not found, skipping aggregation"
        return
    fi

    # Detect format: oha uses "summary.requestsPerSec", wrk uses "throughput.requests_per_sec"
    local first_file
    first_file=$(ls "$runs_dir"/run_*.json 2>/dev/null | head -1)
    local is_oha
    is_oha=$(jq -r 'if .summary.requestsPerSec then "yes" else "no" end' "$first_file" 2>/dev/null || echo "no")

    local rps_values p50_values p99_values rps_min rps_max

    if [[ "$is_oha" == "yes" ]]; then
        # oha format: latency in seconds, need to convert to ms
        rps_values=$(jq -s '[.[].summary.requestsPerSec] | add / length' "$runs_dir"/run_*.json 2>/dev/null || echo "0")
        p50_values=$(jq -s '[.[].latencyPercentiles.p50 * 1000] | add / length' "$runs_dir"/run_*.json 2>/dev/null || echo "0")
        p99_values=$(jq -s '[.[].latencyPercentiles.p99 * 1000] | add / length' "$runs_dir"/run_*.json 2>/dev/null || echo "0")
        rps_min=$(jq -s '[.[].summary.requestsPerSec] | min' "$runs_dir"/run_*.json 2>/dev/null || echo "0")
        rps_max=$(jq -s '[.[].summary.requestsPerSec] | max' "$runs_dir"/run_*.json 2>/dev/null || echo "0")
    else
        # wrk format: latency already in ms
        rps_values=$(jq -s '[.[].throughput.requests_per_sec] | add / length' "$runs_dir"/run_*.json 2>/dev/null || echo "0")
        p50_values=$(jq -s '[.[].latency.percentiles.p50_ms] | add / length' "$runs_dir"/run_*.json 2>/dev/null || echo "0")
        p99_values=$(jq -s '[.[].latency.percentiles.p99_ms] | add / length' "$runs_dir"/run_*.json 2>/dev/null || echo "0")
        rps_min=$(jq -s '[.[].throughput.requests_per_sec] | min' "$runs_dir"/run_*.json 2>/dev/null || echo "0")
        rps_max=$(jq -s '[.[].throughput.requests_per_sec] | max' "$runs_dir"/run_*.json 2>/dev/null || echo "0")
    fi

    cat > "$result_dir/aggregated.json" <<EOF
{
  "runs_count": $RUNS,
  "throughput": {
    "avg_rps": $rps_values,
    "min_rps": $rps_min,
    "max_rps": $rps_max
  },
  "latency": {
    "avg_p50_ms": $p50_values,
    "avg_p99_ms": $p99_values
  }
}
EOF
}

# =============================================================================
# Metadata Generation
# =============================================================================

generate_metadata() {
    local result_dir="$1"
    local proxy="$2"
    local scenario="$3"
    local start_time="$4"
    local end_time="$5"

    local container="bench-${proxy}"
    local proxy_version
    proxy_version=$(get_proxy_version "$proxy")
    local tool_version
    tool_version=$(get_tool_version "$TOOL")

    # Get container image details
    local image_name image_id image_arch
    image_name=$(docker inspect --format='{{.Config.Image}}' "$container" 2>/dev/null || echo "unknown")
    image_id=$(docker inspect --format='{{.Image}}' "$container" 2>/dev/null | cut -c8-19 || echo "unknown")
    image_arch=$(docker image inspect --format='{{.Architecture}}' "$image_name" 2>/dev/null || echo "unknown")

    cat > "$result_dir/metadata.json" <<EOF
{
  "schema_version": "1.0.0",
  "id": "$(uuidgen 2>/dev/null || cat /proc/sys/kernel/random/uuid 2>/dev/null || date +%s%N)",
  "timestamp": {
    "start": "$start_time",
    "end": "$end_time",
    "timezone": "$(date +%Z)"
  },
  "benchmark": {
    "proxy": "$proxy",
    "scenario": "$scenario",
    "name": "${BENCH_NAME:-}",
    "config_file": "configs/${proxy}/${scenario}.kdl"
  },
  "proxy_info": {
    "name": "$proxy",
    "version": "$proxy_version",
    "container": "bench-${proxy}",
    "image": "$image_name",
    "image_id": "$image_id",
    "architecture": "$image_arch"
  },
  "environment": {
    "hostname": "$(hostname)",
    "machine_model": "$(get_machine_model)",
    "os": {
      "name": "$(get_os_info)",
      "version": "$(get_os_version)",
      "kernel": "$(get_kernel_version)"
    },
    "cpu": {
      "model": "$(get_cpu_info)",
      "cores": $(get_cpu_cores),
      "architecture": "$(get_architecture)"
    },
    "memory_gb": $(get_memory_gb),
    "container_runtime": "$(docker --version 2>/dev/null | head -1 || echo 'unknown')"
  },
  "test_parameters": {
    "duration_seconds": $DURATION,
    "warmup_seconds": $WARMUP,
    "connections": $CONNECTIONS,
    "threads": $THREADS,
    "target_rate": $RATE,
    "runs": $RUNS,
    "target_url": "$TARGET"
  },
  "load_generator": {
    "tool": "$TOOL",
    "version": "$tool_version"
  },
  "runs": [
$(for i in $(seq 1 "$RUNS"); do
    echo "    {\"run\": $i, \"output\": \"runs/run_${i}.txt\", \"data\": \"runs/run_${i}.json\"}$( [ $i -lt $RUNS ] && echo ',')"
done)
  ]
}
EOF
}

generate_summary() {
    local result_dir="$1"
    local proxy="$2"
    local scenario="$3"
    local start_time="$4"

    local proxy_version
    proxy_version=$(get_proxy_version "$proxy")

    cat > "$result_dir/summary.md" <<EOF
# Benchmark Results

## Overview

| Property | Value |
|----------|-------|
| **Proxy** | $proxy |
| **Version** | $proxy_version |
| **Scenario** | $scenario |
| **Date** | $start_time |
| **Machine** | $(get_machine_model) |

## Environment

| Property | Value |
|----------|-------|
| **OS** | $(get_os_info) $(get_os_version) |
| **Kernel** | $(get_kernel_version) |
| **CPU** | $(get_cpu_info) |
| **Cores** | $(get_cpu_cores) |
| **Memory** | $(get_memory_gb) GB |
| **Architecture** | $(get_architecture) |

## Test Parameters

| Parameter | Value |
|-----------|-------|
| **Duration** | ${DURATION}s |
| **Warmup** | ${WARMUP}s |
| **Connections** | $CONNECTIONS |
| **Target Rate** | $RATE RPS |
| **Runs** | $RUNS |
| **Load Generator** | $TOOL $(get_tool_version "$TOOL") |

## Results

See individual run files in \`runs/\` directory:

$(for i in $(seq 1 "$RUNS"); do
    echo "- [Run $i](runs/run_${i}.txt)"
done)

## Raw Data

- [metadata.json](metadata.json) - Machine-readable metadata
$(for i in $(seq 1 "$RUNS"); do
    echo "- [runs/run_${i}.json](runs/run_${i}.json) - Run $i data"
done)
EOF
}

# =============================================================================
# Main Benchmark Runner
# =============================================================================

run_benchmark() {
    local proxy="$1"
    local scenario="$2"

    log_info "Starting benchmark: proxy=$proxy scenario=$scenario tool=$TOOL"

    if ! check_proxy_running; then
        log_error "Cannot reach proxy. Start it first with:"
        log_error "  mise run up $proxy"
        exit 1
    fi

    # Show container details for verification
    show_container_info "$proxy"

    # Create result directory with timestamp
    local timestamp
    timestamp=$(date +%Y%m%d_%H%M%S)
    local start_time
    start_time=$(date -Iseconds)
    local result_dir="$RESULTS_DIR/${timestamp}_${proxy}_${scenario}"
    mkdir -p "$result_dir/runs"

    log_info "Results will be saved to: $result_dir"

    run_warmup "$TOOL"

    # Run benchmarks
    for i in $(seq 1 "$RUNS"); do
        local output_file="$result_dir/runs/run_${i}.txt"
        local json_file="$result_dir/runs/run_${i}.json"

        log_info "Run $i of $RUNS"

        case "$TOOL" in
            oha)   run_benchmark_oha "$output_file" "$json_file" ;;
            wrk2)  run_benchmark_wrk2 "$output_file" "$json_file" ;;
            wrk)   run_benchmark_wrk "$output_file" "$json_file" ;;
            hey)   run_benchmark_hey "$output_file" "$json_file" ;;
            k6)    run_benchmark_k6 "$output_file" "$json_file" ;;
        esac

        if [ "$i" -lt "$RUNS" ]; then
            log_info "Cooling down for 5s..."
            sleep 5
        fi
    done

    local end_time
    end_time=$(date -Iseconds)

    # Aggregate results from all runs
    aggregate_results "$result_dir"

    # Generate metadata and summary
    generate_metadata "$result_dir" "$proxy" "$scenario" "$start_time" "$end_time"
    generate_summary "$result_dir" "$proxy" "$scenario" "$start_time"

    log_success "Benchmark complete!"
    log_success "Results: $result_dir"
    log_success "  - metadata.json (machine-readable)"
    log_success "  - summary.md (human-readable)"
    log_success "  - runs/ (individual run data)"
}

run_all_proxies() {
    local scenario="$1"
    local proxies=("sentinel" "envoy" "haproxy" "nginx")

    log_info "Running comparison benchmark for all proxies"

    for proxy in "${proxies[@]}"; do
        log_info "=== Testing $proxy ==="

        log_info "Starting $proxy..."
        # Use --force-recreate to ensure container uses current image (e.g., SENTINEL_IMAGE)
        # Use --pull never for local builds (pull manually if you need registry updates)
        docker-compose -f "$ROOT_DIR/docker/docker-compose.yaml" --profile "$proxy" up -d --force-recreate --pull never
        sleep 5

        run_benchmark "$proxy" "$scenario"

        log_info "Stopping $proxy..."
        docker-compose -f "$ROOT_DIR/docker/docker-compose.yaml" --profile "$proxy" down

        log_info "Cooling down for 10s..."
        sleep 10
    done

    log_success "All benchmarks complete. Results in: $RESULTS_DIR"
}

# =============================================================================
# Parse Arguments
# =============================================================================

PROXY=""
SCENARIO=""
TOOL=""
BENCH_NAME=""

while [[ $# -gt 0 ]]; do
    case $1 in
        --duration)    DURATION="$2"; shift 2 ;;
        --connections) CONNECTIONS="$2"; shift 2 ;;
        --threads)     THREADS="$2"; shift 2 ;;
        --rate)        RATE="$2"; shift 2 ;;
        --warmup)      WARMUP="$2"; shift 2 ;;
        --runs)        RUNS="$2"; shift 2 ;;
        --target)      TARGET="$2"; shift 2 ;;
        --tool)        TOOL="$2"; shift 2 ;;
        --name)        BENCH_NAME="$2"; shift 2 ;;
        --help|-h)     usage ;;
        -*)            log_error "Unknown option: $1"; usage ;;
        *)
            if [ -z "$PROXY" ]; then
                PROXY="$1"
            elif [ -z "$SCENARIO" ]; then
                SCENARIO="$1"
            fi
            shift
            ;;
    esac
done

if [ -z "$PROXY" ] || [ -z "$SCENARIO" ]; then
    log_error "Missing required arguments: proxy and scenario"
    usage
fi

# Auto-detect tool if not specified
if [ -z "$TOOL" ]; then
    TOOL=$(detect_tool)
fi

mkdir -p "$RESULTS_DIR"

if [ "$PROXY" = "all" ]; then
    run_all_proxies "$SCENARIO"
else
    run_benchmark "$PROXY" "$SCENARIO"
fi
